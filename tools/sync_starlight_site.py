#!/usr/bin/env python3
import argparse
import json
import os
import re
import shutil
import tempfile
from pathlib import Path


LINK_RE = re.compile(r"\]\(([^)]+)\)")
GENERATED_BANNER_RE = re.compile(
    r"(?m)^_Generated by `tools/gen_overview_pages\.py`\.\s*Edit only inside `MANUAL:\*` blocks\._\s*$\n?"
)
OCCT_CODE_REF_RE = re.compile(r"(?<!\[)`(occt/src/[^`]+?)`")


def detect_occt_tag(repo_root: Path) -> str:
    """
    Best-effort: extract an OCCT tag from any repro oracle JSON.
    Prefers meta.occt_version, falls back to top-level occt_version.
    """
    for p in sorted((repo_root / "repros").glob("*/golden/*.json")):
        try:
            obj = json.loads(p.read_text(encoding="utf-8", errors="replace"))
        except Exception:
            continue
        version = ""
        if isinstance(obj, dict):
            meta = obj.get("meta")
            if isinstance(meta, dict):
                version = str(meta.get("occt_version") or "")
            if not version:
                version = str(obj.get("occt_version") or "")
        version = version.strip()
        if version:
            # OCCT tags are named like V7_9_3.
            return "V" + version.replace(".", "_")
    # Fall back to a reasonable default; links are still useful even if the tag isn't exact.
    return "master"


def link_occt_code_refs(md: str, *, occt_tag: str) -> str:
    base = f"https://github.com/Open-Cascade-SAS/OCCT/blob/{occt_tag}/"

    def repl(match: re.Match[str]) -> str:
        raw = match.group(1).strip()
        # Convert `occt/src/Foo.cxx` -> upstream `src/Foo.cxx`.
        rel = raw[len("occt/") :] if raw.startswith("occt/") else raw

        suffix = ""
        if "#" in rel:
            rel, frag = rel.split("#", 1)
            suffix = "#" + frag
        else:
            m = re.search(r":(\d+)$", rel)
            if m:
                rel = rel[: m.start()]
                suffix = f"#L{m.group(1)}"

        url = base + rel + suffix
        return f"[`{raw}`]({url})"

    return OCCT_CODE_REF_RE.sub(repl, md)


def rewrite_internal_markdown_links(md: str, *, from_rel: str) -> str:
    """
    Rewrite markdown links for Starlight:
    - Convert .md -> trailing slash routes
    - Convert */README.md -> */readme/
    - Rewrite repo-root style links (notes/.../foo.md) into correct relative links
      within the generated site docs tree.
    """

    from_dir = Path(from_rel).parent.as_posix()
    if from_dir == ".":
        from_dir = ""

    def repo_path_to_site_rel(base: str) -> str | None:
        # Map repo-root paths to the generated docs tree under `occt/`.
        # Returned path is relative to the `occt/` docs root.
        if base.startswith("notes/"):
            return base[len("notes/") :]
        if base.startswith("backlog/docs/"):
            return "backlog/docs/" + base[len("backlog/docs/") :]
        if base.startswith("repros/"):
            return base
        return None

    def rel_to_from_dir(site_rel: str) -> str:
        # Make a relative link from the current doc to the target doc.
        if not from_dir:
            return site_rel
        return Path(os.path.relpath(site_rel, start=from_dir)).as_posix()

    def rewrite_target(target: str) -> str:
        if "://" in target:
            return target
        if target.startswith(("mailto:", "tel:", "#")):
            return target

        base = target
        suffix = ""
        for sep in ("#", "?"):
            if sep in base:
                base, rest = base.split(sep, 1)
                suffix = sep + rest
                break

        # Rewrite repo-root style links into site-relative paths.
        mapped = repo_path_to_site_rel(base)
        if mapped is not None:
            base = rel_to_from_dir(mapped)

        if base.endswith("/README.md") or base.endswith("/readme.md"):
            base = base[: -len("/README.md")] + "/readme/"
        elif base.endswith(".md"):
            base = base[: -len(".md")] + "/"

        return base + suffix

    def repl(match: re.Match[str]) -> str:
        target = match.group(1).strip()
        return f"]({rewrite_target(target)})"

    return LINK_RE.sub(repl, md)


def strip_section(md: str, heading: str) -> str:
    """
    Remove a markdown section by exact heading line (e.g. "## Backlog tasks"),
    up to (but not including) the next heading of same or higher level.
    """
    lines = md.splitlines(keepends=True)
    i = 0
    while i < len(lines):
        if lines[i].rstrip("\n") != heading:
            i += 1
            continue

        level = len(heading.split(" ", 1)[0])  # number of '#'
        start = i
        i += 1
        while i < len(lines):
            line = lines[i]
            if line.startswith("#"):
                hashes = len(line) - len(line.lstrip("#"))
                if hashes <= level and line[hashes : hashes + 1] == " ":
                    break
            i += 1

        del lines[start:i]
        # trim extra blank lines where the section was removed
        while start < len(lines) and lines[start].strip() == "":
            del lines[start]
        if start > 0 and lines[start - 1].strip() == "":
            while start < len(lines) and lines[start].strip() == "":
                del lines[start]
        i = start

    return "".join(lines)


def copy_file(
    src: Path,
    dst: Path,
    *,
    tmp_root: Path,
    src_rel: str,
    dst_rel: str,
    occt_tag: str,
    keep: set[Path] | None = None,
) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    text = src.read_text(encoding="utf-8", errors="replace")
    text = rewrite_internal_markdown_links(text, from_rel=dst_rel)
    text = link_occt_code_refs(text, occt_tag=occt_tag)

    # Site-specific cleanup for generated scaffolding.
    text = GENERATED_BANNER_RE.sub("", text)
    if src_rel.startswith("maps/hub-"):
        # Hubs are generated index pages; keep the “meat” and drop redundant artifact pointers.
        text = strip_section(text, "## Artifacts")
        text = strip_section(text, "## Backlog")

    if not text.startswith("---\n"):
        title = None
        for line in text.splitlines():
            line = line.strip()
            if line.startswith("# "):
                title = line[len("# ") :].strip()
                break
        if not title:
            title = dst.stem.replace("-", " ").strip() or "Document"
        # YAML frontmatter accepts JSON scalars; JSON encoding avoids quoting pitfalls.
        text = f"---\ntitle: {json.dumps(title)}\n---\n\n{text}"

    # Starlight will render the frontmatter title as the page heading.
    # Many repo docs also include an H1 (`# ...`) as the first line, which would
    # otherwise show the title twice. Strip the first H1 after frontmatter.
    if text.startswith("---\n"):
        end = text.find("\n---\n", 4)
        if end != -1:
            frontmatter = text[: end + len("\n---\n")]
            body = text[end + len("\n---\n") :]
            body_lines = body.splitlines(keepends=True)
            for i, line in enumerate(body_lines):
                if line.startswith("# "):
                    del body_lines[i]
                    if i < len(body_lines) and body_lines[i].strip() == "":
                        del body_lines[i]
                    body = "".join(body_lines)
                    text = frontmatter + body
                    break

    # Site-specific pruning: keep backlog tracking out of the rendered docs.
    text = strip_section(text, "## Backlog tasks")

    # Avoid touching the file if nothing changed; helps Astro watchers and keeps diffs clean.
    if dst.is_file():
        try:
            existing = dst.read_text(encoding="utf-8", errors="replace")
        except Exception:
            existing = None
        if existing == text:
            if keep is not None:
                keep.add(dst.resolve())
            return

    # Write atomically to avoid transient partial reads during Astro's file-watching.
    # Keep temp files OUT of the docs tree so Starlight doesn't accidentally index them.
    tmp_root.mkdir(parents=True, exist_ok=True)
    target_mode = 0o644
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="w",
            encoding="utf-8",
            dir=tmp_root,
            prefix="sync-",
            suffix=".tmp",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            f.write(text)
        os.chmod(tmp_path, target_mode)
        os.replace(tmp_path, dst)
        if keep is not None:
            keep.add(dst.resolve())
    finally:
        if tmp_path and tmp_path.exists():
            tmp_path.unlink(missing_ok=True)


def copy_glob(
    root: Path,
    pattern: str,
    dest_root: Path,
    *,
    tmp_root: Path,
    occt_tag: str,
    dst_rel_prefix: str = "",
    keep: set[Path] | None = None,
) -> int:
    count = 0
    for src in sorted(root.glob(pattern)):
        if not src.is_file():
            continue
        rel = src.relative_to(root)
        copy_file(
            src,
            dest_root / rel,
            tmp_root=tmp_root,
            src_rel=str(rel),
            dst_rel=str((Path(dst_rel_prefix) / rel).as_posix()) if dst_rel_prefix else str(rel),
            occt_tag=occt_tag,
            keep=keep,
        )
        count += 1
    return count


def write_text_atomic(dst: Path, text: str, *, tmp_root: Path, keep: set[Path] | None = None) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp_root.mkdir(parents=True, exist_ok=True)

    if dst.is_file():
        try:
            existing = dst.read_text(encoding="utf-8", errors="replace")
        except Exception:
            existing = None
        if existing == text:
            if keep is not None:
                keep.add(dst.resolve())
            return

    target_mode = 0o644
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="w",
            encoding="utf-8",
            dir=tmp_root,
            prefix="sync-",
            suffix=".tmp",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            f.write(text)
        os.chmod(tmp_path, target_mode)
        os.replace(tmp_path, dst)
        if keep is not None:
            keep.add(dst.resolve())
    finally:
        if tmp_path and tmp_path.exists():
            tmp_path.unlink(missing_ok=True)


def write_bytes_atomic(dst: Path, data: bytes, *, tmp_root: Path, keep: set[Path] | None = None) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp_root.mkdir(parents=True, exist_ok=True)

    if dst.is_file():
        try:
            existing = dst.read_bytes()
        except Exception:
            existing = None
        if existing == data:
            if keep is not None:
                keep.add(dst.resolve())
            return

    target_mode = 0o644
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(
            mode="wb",
            dir=tmp_root,
            prefix="sync-",
            suffix=".tmp",
            delete=False,
        ) as f:
            tmp_path = Path(f.name)
            f.write(data)
        os.chmod(tmp_path, target_mode)
        os.replace(tmp_path, dst)
        if keep is not None:
            keep.add(dst.resolve())
    finally:
        if tmp_path and tmp_path.exists():
            tmp_path.unlink(missing_ok=True)


def sync_tree(src_root: Path, dst_root: Path, *, tmp_root: Path) -> None:
    """
    Mirror a directory tree into dst_root, copying only changed files and deleting stale files.
    """
    if not src_root.is_dir():
        return

    want: set[str] = set()
    for src in src_root.rglob("*"):
        if not src.is_file():
            continue
        rel = src.relative_to(src_root).as_posix()
        want.add(rel)
        write_bytes_atomic(dst_root / rel, src.read_bytes(), tmp_root=tmp_root)

    if not dst_root.exists():
        return

    for dst in dst_root.rglob("*"):
        if not dst.is_file():
            continue
        rel = dst.relative_to(dst_root).as_posix()
        if rel not in want:
            dst.unlink()


def write_fillets_oracle_explorer(
    *,
    repo_root: Path,
    site_root: Path,
    dest_root: Path,
    tmp_root: Path,
    occt_tag: str,
    keep: set[Path] | None = None,
) -> None:
    oracle_path = repo_root / "repros" / "lane-fillets" / "golden" / "fillets.json"
    if not oracle_path.is_file():
        return

    # Publish the oracle JSON as a static asset for client-side loading.
    public_oracle = site_root / "public" / "occt" / "oracles" / "fillets.json"
    write_text_atomic(
        public_oracle,
        oracle_path.read_text(encoding="utf-8", errors="replace"),
        tmp_root=tmp_root,
    )

    # Publish per-case mesh artifacts (generated by the repro) as static assets.
    src_artifacts = repo_root / "repros" / "lane-fillets" / "golden" / "artifacts"
    dst_artifacts = site_root / "public" / "occt" / "artifacts" / "fillets"
    sync_tree(src_artifacts, dst_artifacts, tmp_root=tmp_root)

    # Generate an MDX page that mounts an Astro component (scripts in Markdown are not reliably executed).
    mdx = """---
title: "Fillets Oracle Explorer"
tableOfContents: false
---

import FilletsOracleExplorer from '../../../../components/FilletsOracleExplorer.astro';

This page visualizes the fillets repro oracle:
- JSON: `/occt/oracles/fillets.json`
- Source: `repros/lane-fillets/golden/fillets.json`

<FilletsOracleExplorer url="/occt/oracles/fillets.json" />
"""

    # Ensure we don't keep an old .md version around (would cause duplicate ids).
    stale_md = dest_root / "walkthroughs" / "fillets-explorer.md"
    if stale_md.exists():
        stale_md.unlink()

    dst = dest_root / "walkthroughs" / "fillets-explorer.mdx"
    write_text_atomic(dst, mdx, tmp_root=tmp_root, keep=keep)


def write_chfids_model_explorer(
    *,
    repo_root: Path,
    site_root: Path,
    dest_root: Path,
    tmp_root: Path,
    keep: set[Path] | None = None,
) -> None:
    """
    Generate a site-only page that visualizes per-case ChFiDS model artifacts.

    The page reads the oracle JSON for case ids, then loads:
      - public/occt/artifacts/fillets/<case>/model.json
    """
    mdx = """---
title: "ChFiDS Model Explorer"
tableOfContents: false
---

import ChFiDSModelExplorer from '../../../../components/ChFiDSModelExplorer.astro';

This page visualizes a ChFiDS data-model export (work-in-progress).

<ChFiDSModelExplorer oracleUrl="/occt/oracles/fillets.json" />
"""

    dst = dest_root / "walkthroughs" / "chfids-model-explorer.mdx"
    write_text_atomic(dst, mdx, tmp_root=tmp_root, keep=keep)


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", default=".", help="repo root (default: .)")
    ap.add_argument("--site", default="site", help="starlight site dir (default: site)")
    ap.add_argument(
        "--dest-subdir",
        default="occt",
        help="subdirectory under site docs to populate (default: occt)",
    )
    ap.add_argument(
        "--clean",
        action="store_true",
        help="delete and recreate destination subdir before syncing",
    )
    args = ap.parse_args()

    repo_root = Path(args.root).resolve()
    site_root = (repo_root / args.site).resolve()

    docs_root = site_root / "src" / "content" / "docs"
    if not docs_root.is_dir():
        raise SystemExit(f"Missing Starlight docs dir: {docs_root}")

    dest_root = docs_root / args.dest_subdir
    dest_root.mkdir(parents=True, exist_ok=True)
    tmp_root = site_root / ".sync_tmp"
    occt_tag = detect_occt_tag(repo_root)
    keep: set[Path] | None = set() if args.clean else None

    # Landing page for the OCCT section.
    overview_src = repo_root / "notes" / "overview.md"
    overview_dst = dest_root / "index.md"
    if not overview_src.is_file():
        raise SystemExit(f"Missing source overview: {overview_src}")
    copy_file(
        overview_src,
        overview_dst,
        tmp_root=tmp_root,
        src_rel="overview.md",
        dst_rel="index.md",
        occt_tag=occt_tag,
        keep=keep,
    )

    copied = 0
    copied += copy_glob(repo_root / "notes", "maps/*.md", dest_root, tmp_root=tmp_root, occt_tag=occt_tag, keep=keep)
    copied += copy_glob(repo_root / "notes", "dossiers/*.md", dest_root, tmp_root=tmp_root, occt_tag=occt_tag, keep=keep)
    copied += copy_glob(
        repo_root / "notes" / "walkthroughs",
        "*.md",
        dest_root / "walkthroughs",
        tmp_root=tmp_root,
        occt_tag=occt_tag,
        dst_rel_prefix="walkthroughs",
        keep=keep,
    )
    copied += copy_glob(
        repo_root / "repros",
        "*/README.md",
        dest_root / "repros",
        tmp_root=tmp_root,
        occt_tag=occt_tag,
        dst_rel_prefix="repros",
        keep=keep,
    )
    copied += copy_glob(
        repo_root / "backlog",
        "docs/*.md",
        dest_root / "backlog",
        tmp_root=tmp_root,
        occt_tag=occt_tag,
        dst_rel_prefix="backlog",
        keep=keep,
    )

    # Site-only generated interactive pages (not present in repo docs).
    try:
        write_fillets_oracle_explorer(
            repo_root=repo_root,
            site_root=site_root,
            dest_root=dest_root,
            tmp_root=tmp_root,
            occt_tag=occt_tag,
            keep=keep,
        )
    except Exception:
        # Don't fail sync if the interactive helper generation fails; the core docs are more important.
        pass

    try:
        write_chfids_model_explorer(
            repo_root=repo_root,
            site_root=site_root,
            dest_root=dest_root,
            tmp_root=tmp_root,
            keep=keep,
        )
    except Exception:
        pass

    if keep is not None:
        # Delete stale docs after we've written the fresh tree, to avoid transient missing slugs in dev.
        for p in dest_root.rglob("*"):
            if not p.is_file():
                continue
            rp = p.resolve()
            if rp not in keep:
                p.unlink()

    print(f"[ok] Synced {copied + 1} markdown files into {dest_root} (occt tag: {occt_tag})")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
